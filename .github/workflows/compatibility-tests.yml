name: Compatibility Tests

on:
  pull_request:
    branches: [main, develop]
  push:
    branches: [main, develop]
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      litellm_branch:
        description: 'LiteLLM branch to test against'
        required: false
        default: 'main'

permissions:
  contents: write
  pull-requests: write

jobs:
  compatibility-matrix:
    name: Python ${{ matrix.python-version }} / ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ['3.9', '3.10', '3.11', '3.12', '3.13']

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python ${{ matrix.python-version }}
        run: uv python install ${{ matrix.python-version }}

      - name: Set up Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache Rust dependencies
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "compatibility-${{ matrix.os }}"
          cache-targets: false

      - name: Create virtual environment
        run: uv venv --python ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          uv add --dev maturin pytest pytest-asyncio
          uv add litellm

      - name: Build Fast LiteLLM
        id: build
        run: uv run maturin develop --release

      - name: Run compatibility tests
        id: tests
        run: |
          uv run pytest tests/ -v --tb=short -p no:cov --junit-xml=test-results.xml

      - name: Collect compatibility info
        id: info
        shell: bash
        run: |
          uv run python -c "
          import json
          import platform
          import sys
          from importlib.metadata import version

          import fast_litellm

          info = {
              'python_version': f'{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}',
              'platform': platform.system(),
              'platform_version': platform.version(),
              'architecture': platform.machine(),
              'fast_litellm_version': fast_litellm.__version__,
              'litellm_version': version('litellm'),
              'rust_available': fast_litellm.RUST_ACCELERATION_AVAILABLE,
              'health_check': fast_litellm.health_check(),
          }

          # Write to file for artifact
          with open('compatibility-info.json', 'w') as f:
              json.dump(info, f, indent=2, default=str)

          print(json.dumps(info, indent=2, default=str))
          "

      - name: Upload test results
        if: failure() || cancelled()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.os }}-py${{ matrix.python-version }}
          path: |
            test-results.xml
            compatibility-info.json

  generate-report:
    name: Generate Compatibility Report
    needs: compatibility-matrix
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: |
          uv python install 3.11
          uv venv --python 3.11

      - name: Generate compatibility report
        run: |
          uv run python << 'EOF'
          import json
          import os
          from datetime import datetime, timezone
          from pathlib import Path
          from importlib.metadata import version
          import platform

          import fast_litellm

          # Collect all compatibility info
          results = []
          artifacts_dir = Path("artifacts")

          # Check if artifacts directory exists and has content
          if artifacts_dir.exists() and artifacts_dir.is_dir():
              for artifact_dir in artifacts_dir.iterdir():
                  if artifact_dir.is_dir():
                      info_file = artifact_dir / "compatibility-info.json"
                      test_file = artifact_dir / "test-results.xml"

                      if info_file.exists():
                          with open(info_file) as f:
                              info = json.load(f)

                          # Parse test results if available
                          test_passed = True
                          test_count = 0
                          if test_file.exists():
                              import xml.etree.ElementTree as ET
                              try:
                                  tree = ET.parse(test_file)
                                  root = tree.getroot()
                                  testsuite = root.find('.//testsuite') or root
                                  test_count = int(testsuite.get('tests', 0))
                                  failures = int(testsuite.get('failures', 0))
                                  errors = int(testsuite.get('errors', 0))
                                  test_passed = (failures + errors) == 0
                              except Exception as e:
                                  print(f"Warning: Could not parse {test_file}: {e}")

                          info['tests_passed'] = test_passed
                          info['test_count'] = test_count
                          results.append(info)

          # If no results from artifacts, add current environment as the only tested config
          if not results:
              print("No artifacts found - adding current environment info")
              current_info = {
                  'python_version': f'{platform.python_version()}',
                  'platform': platform.system(),
                  'platform_version': platform.version(),
                  'architecture': platform.machine(),
                  'fast_litellm_version': fast_litellm.__version__,
                  'litellm_version': version('litellm'),
                  'rust_available': fast_litellm.RUST_ACCELERATION_AVAILABLE,
                  'health_check': fast_litellm.health_check(),
                  'tests_passed': True,
                  'test_count': 0,
              }
              results.append(current_info)

          # Aggregate results by platform
          platforms = {}
          for r in results:
              p = r['platform']
              if p not in platforms:
                  platforms[p] = {
                      'versions': [],
                      'passed': 0,
                      'failed': 0,
                      'total_tests': 0,
                      'fast_litellm_version': r.get('fast_litellm_version', 'N/A'),
                      'litellm_version': r.get('litellm_version', 'N/A'),
                      'rust_available': r.get('rust_available', False),
                  }
              platforms[p]['versions'].append(r['python_version'])
              if r.get('tests_passed', False):
                  platforms[p]['passed'] += 1
              else:
                  platforms[p]['failed'] += 1
              platforms[p]['total_tests'] += r.get('test_count', 0)

          # Generate markdown report
          report = []
          report.append("# Compatibility Report")
          report.append("")
          report.append(f"Generated: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}")
          report.append("")

          # Summary table - one row per platform
          report.append("## Test Matrix Results")
          report.append("")
          report.append("| Platform | Python Versions | Fast LiteLLM | LiteLLM | Rust | Tests | Status |")
          report.append("|----------|-----------------|--------------|---------|------|-------|--------|")

          for platform in ['Darwin', 'Linux', 'Windows']:
              if platform in platforms:
                  p = platforms[platform]
                  version_list = ', '.join(sorted(p['versions'], key=lambda v: [int(x) for x in v.split('.')]))
                  rust = "✅" if p.get('rust_available', False) else "❌"
                  tests = p['total_tests'] // len(p['versions']) if p['total_tests'] > 0 else 'N/A'
                  status = "✅ Pass" if p['failed'] == 0 else "❌ Fail"
                  report.append(
                      f"| {platform} | {version_list} | "
                      f"{p['fast_litellm_version']} | "
                      f"{p['litellm_version']} | "
                      f"{rust} | {tests} | {status} |"
                  )

          report.append("")

          # Platform summary
          report.append("## Platform Summary")
          report.append("")

          for platform in ['Darwin', 'Linux', 'Windows']:
              if platform in platforms:
                  p = platforms[platform]
                  total = p['passed'] + p['failed']
                  status = "✅" if p['failed'] == 0 else "⚠️"
                  report.append(f"### {platform} {status}")
                  report.append("")
                  report.append(f"- **Tested Python versions**: {', '.join(sorted(p['versions'], key=lambda v: [int(x) for x in v.split('.')]))}")
                  report.append(f"- **Passed**: {p['passed']}/{total}")
                  if p['failed'] > 0:
                      report.append(f"- **Failed**: {p['failed']}/{total}")
                  report.append("")

          # Python version summary
          report.append("## Python Version Summary")
          report.append("")

          py_versions = {}
          for r in results:
              v = r['python_version']
              if v not in py_versions:
                  py_versions[v] = {'passed': 0, 'failed': 0}
              if r.get('tests_passed', False):
                  py_versions[v]['passed'] += 1
              else:
                  py_versions[v]['failed'] += 1

          report.append("| Python Version | Platforms Passed | Platforms Failed | Status |")
          report.append("|----------------|------------------|------------------|--------|")

          for v in sorted(py_versions.keys(), key=lambda v: [int(x) for x in v.split('.')]):
              data = py_versions[v]
              status = "✅" if data['failed'] == 0 else "❌"
              report.append(f"| {v} | {data['passed']} | {data['failed']} | {status} |")

          report.append("")

          # Dependencies
          if results:
              r = results[0]
              report.append("## Tested Versions")
              report.append("")
              report.append(f"- **Fast LiteLLM**: {r.get('fast_litellm_version', 'N/A')}")
              report.append(f"- **LiteLLM**: {r.get('litellm_version', 'N/A')}")
              report.append("")

          # Write report
          report_content = "\n".join(report)
          with open("COMPATIBILITY.md", "w") as f:
              f.write(report_content)

          print(report_content)
          EOF

      - name: Upload compatibility report
        uses: actions/upload-artifact@v4
        with:
          name: compatibility-report
          path: COMPATIBILITY.md

      - name: Commit COMPATIBILITY.md
        if: github.event_name == 'push' || github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add COMPATIBILITY.md
          git diff --staged --quiet || git commit -m "Update COMPATIBILITY.md [skip ci]"
          git push

      - name: Post report to PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('COMPATIBILITY.md', 'utf8');

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('# Compatibility Report')
            );

            const body = report + '\n\n---\n*This report is automatically generated by the compatibility tests workflow.*';

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }
